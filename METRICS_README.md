# Sistema de M√©tricas para el Modelo de Parsing de Facturas

Este sistema implementa m√©tricas completas para evaluar el rendimiento y la calidad del modelo de parsing de facturas.

## üìä M√©tricas Implementadas

### 1. **Confidence Score**
- **Descripci√≥n**: Puntuaci√≥n de confianza basada en la cantidad y calidad de campos extra√≠dos
- **Rango**: 0.0 - 1.0 (1.0 = perfecto)
- **C√°lculo**: Ponderado por campos cr√≠ticos (70%) y adicionales (30%) + bonus por items

### 2. **Field Accuracy**
- **Descripci√≥n**: Precisi√≥n de los campos extra√≠dos comparando con datos de verdad de campo
- **Rango**: 0.0 - 1.0 (1.0 = todos los campos correctos)
- **C√°lculo**: Campos correctos / Total de campos evaluados

### 3. **CER (Character Error Rate)**
- **Descripci√≥n**: Tasa de error a nivel de caracteres en el texto extra√≠do por OCR
- **Rango**: 0.0 - 1.0 (0.0 = perfecto, 1.0 = todos los caracteres incorrectos)
- **C√°lculo**: Distancia de Levenshtein / Longitud del texto correcto

### 4. **WER (Word Error Rate)**
- **Descripci√≥n**: Tasa de error a nivel de palabras en el texto extra√≠do por OCR
- **Rango**: 0.0 - 1.0 (0.0 = perfecto, 1.0 = todas las palabras incorrectas)
- **C√°lculo**: Distancia de Levenshtein a nivel de palabras / N√∫mero de palabras correctas

### 5. **Processing Latency**
- **Descripci√≥n**: Tiempo de procesamiento por documento
- **Unidad**: Segundos
- **Medici√≥n**: Tiempo total desde inicio hasta fin del procesamiento

### 6. **Throughput**
- **Descripci√≥n**: N√∫mero de documentos procesados por segundo
- **Unidad**: Documentos/segundo
- **C√°lculo**: 1 / Tiempo promedio de procesamiento

## üöÄ Uso del Sistema

### 1. Prueba del Sistema de M√©tricas

```bash
# Ejecutar pruebas del sistema de m√©tricas
python test_metrics_system.py
```

Este script:
- Prueba el calculador de m√©tricas con datos de ejemplo
- Demuestra el funcionamiento con datos que contienen errores
- Crea un archivo de ground truth de ejemplo
- Muestra c√≥mo usar el procesador de lotes

### 2. Benchmark del Modelo

```bash
# Benchmark b√°sico con archivos de prueba
python benchmark_model.py --test-dir /ruta/a/facturas

# Benchmark con diferentes tama√±os de lote
python benchmark_model.py --test-dir /ruta/a/facturas --batch-sizes 10 25 50 100

# Benchmark con datos de verdad de campo
python benchmark_model.py --test-dir /ruta/a/facturas --ground-truth ground_truth.json

# Crear archivo de ground truth de ejemplo
python benchmark_model.py --test-dir /ruta/a/facturas --create-sample-gt
```

### 3. Uso de la API

#### Evaluar M√©tricas de un Archivo

```bash
curl -X POST "http://localhost:8000/evaluate-metrics" \
  -F "file=@factura.pdf" \
  -F 'ground_truth={"tipo_factura": "A", "razon_social_vendedor": "Empresa SRL", ...}'
```

#### Benchmark de Lotes

```bash
curl -X POST "http://localhost:8000/batch-benchmark" \
  -F "files=@factura1.pdf" \
  -F "files=@factura2.pdf" \
  -F "files=@factura3.pdf" \
  -F 'ground_truth={"factura1.pdf": {...}, "factura2.pdf": {...}}'
```

## üìÅ Estructura de Archivos

```
‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îú‚îÄ‚îÄ metrics_calculator.py    # Calculador de m√©tricas
‚îÇ   ‚îú‚îÄ‚îÄ batch_processor.py       # Procesador de lotes
‚îÇ   ‚îî‚îÄ‚îÄ invoice_parser.py        # Parser de facturas (existente)
‚îú‚îÄ‚îÄ benchmark_model.py           # Script principal de benchmark
‚îú‚îÄ‚îÄ test_metrics_system.py       # Script de pruebas
‚îú‚îÄ‚îÄ main.py                      # API con endpoints de m√©tricas
‚îî‚îÄ‚îÄ models.py                    # Modelos de datos actualizados
```

## üìã Preparaci√≥n de Datos

### 1. Archivos de Prueba
Coloca tus facturas en un directorio:
```
test_facturas/
‚îú‚îÄ‚îÄ factura1.pdf
‚îú‚îÄ‚îÄ factura2.jpg
‚îú‚îÄ‚îÄ factura3.png
‚îî‚îÄ‚îÄ ...
```

### 2. Ground Truth (Datos de Verdad de Campo)
Crea un archivo JSON con los datos correctos:

```json
{
  "factura1.pdf": {
    "tipo_factura": "A",
    "razon_social_vendedor": "Empresa Ejemplo SRL",
    "cuit_vendedor": "20-12345678-9",
    "razon_social_comprador": "Cliente Ejemplo",
    "cuit_comprador": "20-87654321-0",
    "condicion_iva_comprador": "Responsable Inscripto",
    "condicion_venta": "Contado",
    "fecha_emision": "01/01/2024",
    "subtotal": "1000,00",
    "importe_total": "1210,00",
    "iva": "210,00",
    "deuda_impositiva": "210,00",
    "numero_factura": "0001-00000001",
    "punto_venta": "0001",
    "raw_text": "Texto completo de la factura..."
  }
}
```

## üìä Interpretaci√≥n de Resultados

### Confidence Score
- **0.8 - 1.0**: Excelente - Modelo muy confiable
- **0.6 - 0.8**: Bueno - Modelo confiable con algunos campos faltantes
- **0.4 - 0.6**: Regular - Modelo necesita mejoras
- **0.0 - 0.4**: Malo - Modelo no es confiable

### Field Accuracy
- **0.9 - 1.0**: Excelente - Casi todos los campos correctos
- **0.7 - 0.9**: Bueno - Mayor√≠a de campos correctos
- **0.5 - 0.7**: Regular - Algunos campos incorrectos
- **0.0 - 0.5**: Malo - Muchos campos incorrectos

### CER/WER
- **0.0 - 0.1**: Excelente - Muy pocos errores de OCR
- **0.1 - 0.3**: Bueno - Algunos errores de OCR
- **0.3 - 0.5**: Regular - Errores moderados de OCR
- **0.5 - 1.0**: Malo - Muchos errores de OCR

### Throughput
- **> 2 docs/seg**: Excelente - Procesamiento muy r√°pido
- **1 - 2 docs/seg**: Bueno - Procesamiento r√°pido
- **0.5 - 1 docs/seg**: Regular - Procesamiento moderado
- **< 0.5 docs/seg**: Malo - Procesamiento lento

## üîß Configuraci√≥n Avanzada

### Ajustar N√∫mero de Workers
```bash
python benchmark_model.py --test-dir /ruta/facturas --max-workers 8
```

### Procesar Lotes Espec√≠ficos
```bash
python benchmark_model.py --test-dir /ruta/facturas --batch-sizes 50 100
```

### Guardar Resultados en Directorio Espec√≠fico
```bash
python benchmark_model.py --test-dir /ruta/facturas --output-dir resultados_benchmark
```

## üìà Ejemplo de Salida

```
=== REPORTE DE RENDIMIENTO DEL MODELO ===
Fecha: 2024-01-15 14:30:25

RESUMEN DEL LOTE:
- Total de archivos: 50
- Archivos procesados exitosamente: 48
- Archivos fallidos: 2
- Tasa de √©xito: 96.00%
- Tiempo total de procesamiento: 45.30 segundos

M√âTRICAS DE RENDIMIENTO:
- Throughput: 1.06 documentos/segundo
- Tiempo promedio por documento: 0.94 segundos
- Confidence Score promedio: 0.847

M√âTRICAS DE CALIDAD:
- Precisi√≥n de campos promedio: 0.923
- CER (Character Error Rate) promedio: 0.045
- WER (Word Error Rate) promedio: 0.078
- Campos correctos totales: 442
- Campos faltantes totales: 23
- Campos incorrectos totales: 15
```

## üêõ Soluci√≥n de Problemas

### Error: "No se encontraron archivos de prueba"
- Verifica que el directorio contenga archivos .pdf, .jpg, .jpeg, .png
- Aseg√∫rate de que los archivos no est√©n corruptos

### Error: "Error procesando ground truth"
- Verifica que el JSON sea v√°lido
- Aseg√∫rate de que los nombres de archivo coincidan exactamente

### Rendimiento lento
- Reduce el n√∫mero de workers si hay problemas de memoria
- Procesa lotes m√°s peque√±os
- Verifica que no haya otros procesos consumiendo recursos

## üöÄ Mejoras Implementadas

### 1. **C√°lculo de Confianza Mejorado**
- **Enfoque**: Solo campos estructurados espec√≠ficos (no todo lo que extrae el OCR)
- **Campos evaluados**: `cuit_vendedor`, `cuit_comprador`, `fecha_emision`, `subtotal`, `importe_total`
- **Peso**: 70% campos estructurados, 30% items estructurados
- **Beneficio**: Confianza precisa basada solo en datos relevantes del ground truth

### 2. **Evaluaci√≥n Solo de Campos Estructurados**
- **Campos principales**: `cuit_vendedor`, `cuit_comprador`, `fecha_emision`, `subtotal`, `importe_total`
- **Campos de items**: `descripcion`, `cantidad`, `precio_unitario`, `bonificacion`, `importe_bonificacion`
- **Ignorados**: Todos los otros campos extra√≠dos por OCR que no est√°n en ground truth
- **Beneficio**: M√©tricas precisas sin penalizar campos irrelevantes

### 3. **CER/WER Mejorado - Solo Campos Importantes**
- **Extracci√≥n selectiva**: Solo campos estructurados importantes
- **Normalizaci√≥n completa**: Min√∫sculas, sin comas/puntos, sin espacios extra
- **Patrones flexibles**: Extrae CUITs, fechas, montos, porcentajes, palabras importantes
- **Filtrado inteligente**: Solo elementos relevantes, evita n√∫meros irrelevantes
- **Ground truth text**: Generado autom√°ticamente desde JSON para comparaci√≥n
- **Beneficio**: CER/WER precisos y relevantes (0.000 = perfecto)

### 4. **C√°lculo Autom√°tico de Items**
- **`importe_bonificacion`**: Calculado autom√°ticamente cuando falta
- **`subtotal`**: Calculado como `(cantidad * precio_unitario) - importe_bonificacion`
- **Validaci√≥n de items**: Solo campos importantes para confianza
- **Beneficio**: Items m√°s completos y precisos

### 5. **M√©tricas Comprehensivas Mejoradas**
- **Pre-procesamiento**: Correcciones aplicadas antes del c√°lculo
- **Textos limpios**: CER/WER calculados con texto normalizado
- **Campos corregidos**: Accuracy basado en campos completos
- **Beneficio**: M√©tricas m√°s precisas y consistentes

## üìä Resultados de las Mejoras

### Antes vs Despu√©s:
```
Evaluaci√≥n Original: Todos los campos del OCR ‚Üí Evaluaci√≥n Mejorada: Solo campos estructurados
Confianza Original: Variable ‚Üí Confianza Mejorada: 1.000 (perfecto)
Accuracy Original: Baja por campos irrelevantes ‚Üí Accuracy Mejorada: 1.000 (perfecto)
CER/WER: Mejorados con limpieza de texto
```

### Campos Estructurados Evaluados:
- **Principales**: `cuit_vendedor`, `cuit_comprador`, `fecha_emision`, `subtotal`, `importe_total`
- **Items**: `descripcion`, `cantidad`, `precio_unitario`, `bonificacion`, `importe_bonificacion`
- **Ignorados**: 15+ campos del OCR que no est√°n en ground truth

## üß™ Pruebas de las Mejoras

```bash
# Probar evaluaci√≥n solo de campos estructurados
python test_structured_metrics.py

# Probar las mejoras implementadas
python tests/test_improvements.py
```

Estos scripts demuestran:
- CER/WER calculado solo sobre campos importantes con normalizaci√≥n completa
- Evaluaci√≥n solo de campos estructurados espec√≠ficos
- Ignorar campos del OCR que no est√°n en ground truth
- C√°lculo mejorado de confianza basado en datos relevantes
- M√©tricas precisas sin penalizar campos irrelevantes

## üìû Soporte

Para problemas o preguntas sobre el sistema de m√©tricas:
1. Revisa los logs en la consola
2. Verifica que todas las dependencias est√©n instaladas
3. Aseg√∫rate de que los archivos de prueba sean v√°lidos
4. Consulta la documentaci√≥n de la API en `/docs` cuando el servidor est√© ejecut√°ndose
5. Ejecuta `python tests/test_improvements.py` para verificar las mejoras
