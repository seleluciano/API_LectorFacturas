# Sistema de M√©tricas para el Modelo de Parsing de Facturas

Este sistema implementa m√©tricas completas para evaluar el rendimiento y la calidad del modelo de parsing de facturas.

## üìä M√©tricas Implementadas

### 1. **Confidence Score**
- **Descripci√≥n**: Puntuaci√≥n de confianza basada en la cantidad y calidad de campos extra√≠dos
- **Rango**: 0.0 - 1.0 (1.0 = perfecto)
- **C√°lculo**: Ponderado por campos cr√≠ticos (70%) y adicionales (30%) + bonus por items

### 2. **Field Accuracy**
- **Descripci√≥n**: Precisi√≥n de los campos extra√≠dos comparando con datos de verdad de campo
- **Rango**: 0.0 - 1.0 (1.0 = todos los campos correctos)
- **C√°lculo**: Campos correctos / Total de campos evaluados

### 3. **CER (Character Error Rate)**
- **Descripci√≥n**: Tasa de error a nivel de caracteres en el texto extra√≠do por OCR
- **Rango**: 0.0 - 1.0 (0.0 = perfecto, 1.0 = todos los caracteres incorrectos)
- **C√°lculo**: Distancia de Levenshtein / Longitud del texto correcto

### 4. **WER (Word Error Rate)**
- **Descripci√≥n**: Tasa de error a nivel de palabras en el texto extra√≠do por OCR
- **Rango**: 0.0 - 1.0 (0.0 = perfecto, 1.0 = todas las palabras incorrectas)
- **C√°lculo**: Distancia de Levenshtein a nivel de palabras / N√∫mero de palabras correctas

### 5. **Processing Latency**
- **Descripci√≥n**: Tiempo de procesamiento por documento
- **Unidad**: Segundos
- **Medici√≥n**: Tiempo total desde inicio hasta fin del procesamiento

### 6. **Throughput**
- **Descripci√≥n**: N√∫mero de documentos procesados por segundo
- **Unidad**: Documentos/segundo
- **C√°lculo**: 1 / Tiempo promedio de procesamiento

## üöÄ Uso del Sistema

### 1. Prueba del Sistema de M√©tricas

```bash
# Ejecutar pruebas del sistema de m√©tricas
python test_metrics_system.py
```

Este script:
- Prueba el calculador de m√©tricas con datos de ejemplo
- Demuestra el funcionamiento con datos que contienen errores
- Crea un archivo de ground truth de ejemplo
- Muestra c√≥mo usar el procesador de lotes

### 2. Benchmark del Modelo

```bash
# Benchmark b√°sico con archivos de prueba
python benchmark_model.py --test-dir /ruta/a/facturas

# Benchmark con diferentes tama√±os de lote
python benchmark_model.py --test-dir /ruta/a/facturas --batch-sizes 10 25 50 100

# Benchmark con datos de verdad de campo
python benchmark_model.py --test-dir /ruta/a/facturas --ground-truth ground_truth.json

# Crear archivo de ground truth de ejemplo
python benchmark_model.py --test-dir /ruta/a/facturas --create-sample-gt
```

### 3. Uso de la API

#### Evaluar M√©tricas de un Archivo

```bash
curl -X POST "http://localhost:8000/evaluate-metrics" \
  -F "file=@factura.pdf" \
  -F 'ground_truth={"tipo_factura": "A", "razon_social_vendedor": "Empresa SRL", ...}'
```

#### Benchmark de Lotes

```bash
curl -X POST "http://localhost:8000/batch-benchmark" \
  -F "files=@factura1.pdf" \
  -F "files=@factura2.pdf" \
  -F "files=@factura3.pdf" \
  -F 'ground_truth={"factura1.pdf": {...}, "factura2.pdf": {...}}'
```

## üìÅ Estructura de Archivos

```
‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îú‚îÄ‚îÄ metrics_calculator.py    # Calculador de m√©tricas
‚îÇ   ‚îú‚îÄ‚îÄ batch_processor.py       # Procesador de lotes
‚îÇ   ‚îî‚îÄ‚îÄ invoice_parser.py        # Parser de facturas (existente)
‚îú‚îÄ‚îÄ benchmark_model.py           # Script principal de benchmark
‚îú‚îÄ‚îÄ test_metrics_system.py       # Script de pruebas
‚îú‚îÄ‚îÄ main.py                      # API con endpoints de m√©tricas
‚îî‚îÄ‚îÄ models.py                    # Modelos de datos actualizados
```

## üìã Preparaci√≥n de Datos

### 1. Archivos de Prueba
Coloca tus facturas en un directorio:
```
test_facturas/
‚îú‚îÄ‚îÄ factura1.pdf
‚îú‚îÄ‚îÄ factura2.jpg
‚îú‚îÄ‚îÄ factura3.png
‚îî‚îÄ‚îÄ ...
```

### 2. Ground Truth (Datos de Verdad de Campo)
Crea un archivo JSON con los datos correctos:

```json
{
  "factura1.pdf": {
    "tipo_factura": "A",
    "razon_social_vendedor": "Empresa Ejemplo SRL",
    "cuit_vendedor": "20-12345678-9",
    "razon_social_comprador": "Cliente Ejemplo",
    "cuit_comprador": "20-87654321-0",
    "condicion_iva_comprador": "Responsable Inscripto",
    "condicion_venta": "Contado",
    "fecha_emision": "01/01/2024",
    "subtotal": "1000,00",
    "importe_total": "1210,00",
    "iva": "210,00",
    "deuda_impositiva": "210,00",
    "numero_factura": "0001-00000001",
    "punto_venta": "0001",
    "raw_text": "Texto completo de la factura..."
  }
}
```

## üìä Interpretaci√≥n de Resultados

### Confidence Score
- **0.8 - 1.0**: Excelente - Modelo muy confiable
- **0.6 - 0.8**: Bueno - Modelo confiable con algunos campos faltantes
- **0.4 - 0.6**: Regular - Modelo necesita mejoras
- **0.0 - 0.4**: Malo - Modelo no es confiable

### Field Accuracy
- **0.9 - 1.0**: Excelente - Casi todos los campos correctos
- **0.7 - 0.9**: Bueno - Mayor√≠a de campos correctos
- **0.5 - 0.7**: Regular - Algunos campos incorrectos
- **0.0 - 0.5**: Malo - Muchos campos incorrectos

### CER/WER
- **0.0 - 0.1**: Excelente - Muy pocos errores de OCR
- **0.1 - 0.3**: Bueno - Algunos errores de OCR
- **0.3 - 0.5**: Regular - Errores moderados de OCR
- **0.5 - 1.0**: Malo - Muchos errores de OCR

### Throughput
- **> 2 docs/seg**: Excelente - Procesamiento muy r√°pido
- **1 - 2 docs/seg**: Bueno - Procesamiento r√°pido
- **0.5 - 1 docs/seg**: Regular - Procesamiento moderado
- **< 0.5 docs/seg**: Malo - Procesamiento lento

## üîß Configuraci√≥n Avanzada

### Ajustar N√∫mero de Workers
```bash
python benchmark_model.py --test-dir /ruta/facturas --max-workers 8
```

### Procesar Lotes Espec√≠ficos
```bash
python benchmark_model.py --test-dir /ruta/facturas --batch-sizes 50 100
```

### Guardar Resultados en Directorio Espec√≠fico
```bash
python benchmark_model.py --test-dir /ruta/facturas --output-dir resultados_benchmark
```

## üìà Ejemplo de Salida

```
=== REPORTE DE RENDIMIENTO DEL MODELO ===
Fecha: 2024-01-15 14:30:25

RESUMEN DEL LOTE:
- Total de archivos: 50
- Archivos procesados exitosamente: 48
- Archivos fallidos: 2
- Tasa de √©xito: 96.00%
- Tiempo total de procesamiento: 45.30 segundos

M√âTRICAS DE RENDIMIENTO:
- Throughput: 1.06 documentos/segundo
- Tiempo promedio por documento: 0.94 segundos
- Confidence Score promedio: 0.847

M√âTRICAS DE CALIDAD:
- Precisi√≥n de campos promedio: 0.923
- CER (Character Error Rate) promedio: 0.045
- WER (Word Error Rate) promedio: 0.078
- Campos correctos totales: 442
- Campos faltantes totales: 23
- Campos incorrectos totales: 15
```

## üêõ Soluci√≥n de Problemas

### Error: "No se encontraron archivos de prueba"
- Verifica que el directorio contenga archivos .pdf, .jpg, .jpeg, .png
- Aseg√∫rate de que los archivos no est√©n corruptos

### Error: "Error procesando ground truth"
- Verifica que el JSON sea v√°lido
- Aseg√∫rate de que los nombres de archivo coincidan exactamente

### Rendimiento lento
- Reduce el n√∫mero de workers si hay problemas de memoria
- Procesa lotes m√°s peque√±os
- Verifica que no haya otros procesos consumiendo recursos

## üöÄ Mejoras Implementadas

### 1. **C√°lculo de Confianza Mejorado**
- **Enfoque**: Solo campos estructurados espec√≠ficos (no todo lo que extrae el OCR)
- **Campos evaluados**: `cuit_vendedor`, `cuit_comprador`, `fecha_emision`, `subtotal`, `importe_total`
- **Peso**: 70% campos estructurados, 30% items estructurados
- **Beneficio**: Confianza precisa basada solo en datos relevantes del ground truth

### 2. **Evaluaci√≥n Solo de Campos Estructurados**
- **Campos principales**: `cuit_vendedor`, `cuit_comprador`, `fecha_emision`, `subtotal`, `importe_total`
- **Campos de items**: `descripcion`, `cantidad`, `precio_unitario`, `bonificacion`, `importe_bonificacion`
- **Ignorados**: Todos los otros campos extra√≠dos por OCR que no est√°n en ground truth
- **Beneficio**: M√©tricas precisas sin penalizar campos irrelevantes

### 3. **CER/WER Mejorado - Solo Campos Importantes**
- **Extracci√≥n selectiva**: Solo campos estructurados importantes
- **Normalizaci√≥n completa**: Min√∫sculas, sin comas/puntos, sin espacios extra
- **Patrones flexibles**: Extrae CUITs, fechas, montos, porcentajes, palabras importantes
- **Filtrado inteligente**: Solo elementos relevantes, evita n√∫meros irrelevantes
- **Ground truth text**: Generado autom√°ticamente desde JSON para comparaci√≥n
- **Beneficio**: CER/WER precisos y relevantes (0.000 = perfecto)

### 4. **C√°lculo Autom√°tico de Items**
- **`importe_bonificacion`**: Calculado autom√°ticamente cuando falta
- **`subtotal`**: Calculado como `(cantidad * precio_unitario) - importe_bonificacion`
- **Validaci√≥n de items**: Solo campos importantes para confianza
- **Beneficio**: Items m√°s completos y precisos

### 5. **M√©tricas Comprehensivas Mejoradas**
- **Pre-procesamiento**: Correcciones aplicadas antes del c√°lculo
- **Textos limpios**: CER/WER calculados con texto normalizado
- **Campos corregidos**: Accuracy basado en campos completos
- **Beneficio**: M√©tricas m√°s precisas y consistentes

## üìä Resultados de las Mejoras

### Antes vs Despu√©s:
```
Evaluaci√≥n Original: Todos los campos del OCR ‚Üí Evaluaci√≥n Mejorada: Solo campos estructurados
Confianza Original: Variable ‚Üí Confianza Mejorada: 1.000 (perfecto)
Accuracy Original: Baja por campos irrelevantes ‚Üí Accuracy Mejorada: 1.000 (perfecto)
CER/WER: Mejorados con limpieza de texto
```

### Campos Estructurados Evaluados:
- **Principales**: `cuit_vendedor`, `cuit_comprador`, `fecha_emision`, `subtotal`, `importe_total`
- **Items**: `descripcion`, `cantidad`, `precio_unitario`, `bonificacion`, `importe_bonificacion`
- **Ignorados**: 15+ campos del OCR que no est√°n en ground truth

## üß™ Pruebas de las Mejoras

```bash
# Probar evaluaci√≥n solo de campos estructurados
python test_structured_metrics.py

# Probar las mejoras implementadas
python tests/test_improvements.py
```

Estos scripts demuestran:
- CER/WER calculado solo sobre campos importantes con normalizaci√≥n completa
- Evaluaci√≥n solo de campos estructurados espec√≠ficos
- Ignorar campos del OCR que no est√°n en ground truth
- C√°lculo mejorado de confianza basado en datos relevantes
- M√©tricas precisas sin penalizar campos irrelevantes

## üìû Soporte

Para problemas o preguntas sobre el sistema de m√©tricas:
1. Revisa los logs en la consola
2. Verifica que todas las dependencias est√©n instaladas
3. Aseg√∫rate de que los archivos de prueba sean v√°lidos
4. Consulta la documentaci√≥n de la API en `/docs` cuando el servidor est√© ejecut√°ndose
5. Ejecuta `python tests/test_improvements.py` para verificar las mejoras

## üìã An√°lisis Detallado de Campos

El sistema incluye un **analizador detallado de campos** que permite identificar exactamente qu√© campos est√°n causando problemas de precisi√≥n en el modelo.

### üéØ Funcionalidades del An√°lisis Detallado

#### 1. **Comparaci√≥n con Ground Truth Real**
- Compara los campos extra√≠dos con los datos reales del JSON
- Identifica campos correctos, incorrectos y faltantes
- Proporciona ejemplos espec√≠ficos de errores

#### 2. **Estad√≠sticas por Campo**
- **Precisi√≥n por campo**: Porcentaje de aciertos para cada campo
- **Conteo de errores**: Campos correctos, incorrectos y faltantes
- **Ejemplos de errores**: Casos espec√≠ficos donde el campo fall√≥

#### 3. **An√°lisis por Documento**
- Identifica documentos con mejor y peor rendimiento
- Muestra la precisi√≥n individual de cada factura
- Permite identificar patrones problem√°ticos

#### 4. **Recomendaciones Espec√≠ficas**
- Prioriza campos que requieren mejora
- Identifica si el problema es extracci√≥n o validaci√≥n
- Proporciona recomendaciones concretas de mejora

### üöÄ Uso del An√°lisis Detallado

#### 1. **Generar Reporte de An√°lisis**

```bash
# An√°lisis detallado con ground truth real
python services/detailed_field_analysis.py

# Con par√°metros personalizados
python services/detailed_field_analysis.py \
  --benchmark-file benchmark_results/dataset_benchmark_results.json \
  --dataset-dir "C:\Users\selel\OneDrive\Documentos\Facultad\ARPYME\creacion_dataset\dataset_facturas" \
  --output benchmark_results/detailed_field_analysis.txt
```

#### 2. **Interpretar los Resultados**

El reporte generado incluye:

**üìä Resumen Ejecutivo:**
- Total de documentos analizados
- Total de campos analizados  
- Precisi√≥n promedio general

**‚ö†Ô∏è Campos con Peor Rendimiento (Top 5):**
- Lista de campos que m√°s afectan la precisi√≥n
- Porcentaje de precisi√≥n espec√≠fico
- Ejemplos de errores encontrados

**‚úÖ Campos con Mejor Rendimiento (Top 5):**
- Campos que funcionan correctamente
- Confirmaci√≥n de que el modelo extrae bien ciertos tipos de datos

**üìã An√°lisis Completo por Campo:**
- Estad√≠sticas detalladas de cada campo
- Desglose de errores (correctos, incorrectos, faltantes)
- Ejemplos espec√≠ficos de errores

**üìà An√°lisis por Documento:**
- Documentos con mejor y peor rendimiento
- Identificaci√≥n de facturas problem√°ticas

**üéØ Recomendaciones Espec√≠ficas:**
- Prioridades de mejora
- Tipos de problemas identificados
- Acciones recomendadas

### üìã Ejemplo de Reporte

```
=== REPORTE DETALLADO DE AN√ÅLISIS DE CAMPOS ===
Fecha: 2025-09-19 21:05:06
Total de documentos analizados: 10

RESUMEN EJECUTIVO:
‚Ä¢ Total de campos analizados: 10
‚Ä¢ Precisi√≥n promedio: 0.786 (78.6%)

CAMPOS CON PEOR RENDIMIENTO (Top 5):

1. BONIFICACION:
   Precisi√≥n: 0.000 (0.0%)
   Total evaluado: 28
   Correctos: 0 (0.0%)
   Incorrectos: 0 (0.0%)
   Faltantes: 28 (100.0%)

2. DESCRIPCION:
   Precisi√≥n: 0.714 (71.4%)
   Total evaluado: 28
   Correctos: 20 (71.4%)
   Incorrectos: 8 (28.6%)
   Faltantes: 0 (0.0%)
   Ejemplos de errores:
     ‚Ä¢ factura_11.png: '' vs 'mantenimiento mensual'
     ‚Ä¢ factura_10.png: 'instalaci√≥n servidores' vs 'implementaci√≥n de red'

RECOMENDACIONES ESPEC√çFICAS:
üéØ PRIORIDAD ALTA: Mejorar el campo 'bonificacion'
   ‚Ä¢ Precisi√≥n actual: 0.0%
   ‚Ä¢ Problema principal: CAMPOS FALTANTES (28 de 28)
   ‚Ä¢ Recomendaci√≥n: Mejorar extracci√≥n/reconocimiento de este campo
```

### üîß Configuraci√≥n del An√°lisis

#### Par√°metros Disponibles:

- `--benchmark-file`: Archivo JSON de resultados del benchmark
- `--dataset-dir`: Directorio del dataset con JSONs de ground truth
- `--output`: Archivo de salida del reporte

#### Campos Analizados:

**Campos Principales:**
- `cuit_vendedor`: CUIT del vendedor
- `cuit_comprador`: CUIT del comprador
- `fecha_emision`: Fecha de emisi√≥n
- `subtotal`: Subtotal de la factura
- `importe_total`: Importe total

**Campos de Items:**
- `descripcion`: Descripci√≥n del producto/servicio
- `cantidad`: Cantidad del item
- `precio_unitario`: Precio unitario
- `bonificacion`: Porcentaje de bonificaci√≥n
- `importe_bonificacion`: Importe de la bonificaci√≥n

### üí° Casos de Uso

#### 1. **Identificar Problemas Espec√≠ficos**
```bash
# Despu√©s de ejecutar un benchmark
python services/detailed_field_analysis.py
# Revisar el reporte para identificar campos problem√°ticos
```

#### 2. **Validar Mejoras del Modelo**
```bash
# Antes y despu√©s de mejorar el modelo
python services/detailed_field_analysis.py --output before_improvement.txt
# ... mejorar el modelo ...
python services/detailed_field_analysis.py --output after_improvement.txt
# Comparar los reportes para verificar mejoras
```

#### 3. **An√°lisis de Dataset Espec√≠fico**
```bash
# Analizar un dataset espec√≠fico
python services/detailed_field_analysis.py \
  --dataset-dir "path/to/specific/dataset" \
  --output "analysis_specific_dataset.txt"
```

### üìä Interpretaci√≥n de Resultados

#### **Precisi√≥n por Campo:**
- **100%**: Campo funciona perfectamente
- **80-99%**: Campo funciona bien, mejoras menores
- **60-79%**: Campo necesita mejoras moderadas
- **<60%**: Campo requiere atenci√≥n prioritaria
- **0%**: Campo no funciona, requiere redise√±o

#### **Tipos de Errores:**
- **Campos Faltantes**: El campo no se extrae
- **Campos Incorrectos**: El campo se extrae pero con valor incorrecto
- **Campos Vac√≠os**: El campo se extrae pero est√° vac√≠o

#### **Recomendaciones por Tipo de Error:**
- **Faltantes**: Mejorar patrones de extracci√≥n o reconocimiento OCR
- **Incorrectos**: Mejorar validaci√≥n o l√≥gica de extracci√≥n
- **Vac√≠os**: Revisar condiciones de extracci√≥n

### üîÑ Integraci√≥n con el Workflow

El an√°lisis detallado se integra perfectamente con el workflow de benchmark:

1. **Ejecutar Benchmark**: `python benchmark_dataset.py --dataset-dir ...`
2. **Generar An√°lisis**: `python services/detailed_field_analysis.py`
3. **Revisar Resultados**: Analizar el reporte generado
4. **Implementar Mejoras**: Basado en las recomendaciones
5. **Repetir**: Ejecutar nuevo benchmark para validar mejoras

### üìÅ Archivos Generados

- `benchmark_results/detailed_field_analysis.txt`: Reporte detallado completo
- Incluye an√°lisis por campo, documento y recomendaciones espec√≠ficas

Este sistema de an√°lisis detallado te permite **identificar exactamente qu√© campos est√°n causando problemas** y **priorizar las mejoras** de manera eficiente.

## üìã Documentaci√≥n Adicional

### üìä An√°lisis de Rendimiento de Campos

Para informaci√≥n detallada sobre el rendimiento actual de cada campo:

- **üìà Resumen Ejecutivo**: `FIELD_PERFORMANCE_SUMMARY.md` - Vista r√°pida de campos problem√°ticos y excelentes
- **üìã An√°lisis Completo**: `FIELD_PERFORMANCE_ANALYSIS.md` - An√°lisis detallado con m√©tricas, ejemplos y plan de mejoras
- **üîç Reporte T√©cnico**: `benchmark_results/detailed_field_analysis.txt` - Resultados del √∫ltimo an√°lisis

### üéØ Estado Actual del Sistema

**Precisi√≥n Promedio**: 78.6% | **Estado**: Moderado

**Campos Excelentes** (100% precisi√≥n):
- CUIT Vendedor, CUIT Comprador, Fecha Emisi√≥n, Subtotal, Importe Total

**Campos Problem√°ticos**:
- üö® **Bonificaci√≥n**: 0% (100% campos faltantes)
- üî¥ **Descripci√≥n**: 71.4% (28.6% incorrectos)
- üî¥ **Cantidad**: 71.4% (28.6% incorrectos)
- üî¥ **Precio Unitario**: 71.4% (28.6% incorrectos)
- üî¥ **Importe Bonificaci√≥n**: 71.4% (28.6% incorrectos)

### üöÄ Pr√≥ximos Pasos Recomendados

1. **Prioridad ALTA**: Redise√±ar extracci√≥n de bonificaciones (0% ‚Üí 70%+)
2. **Prioridad MEDIA**: Mejorar campos de items (71.4% ‚Üí 80%+)
3. **Prioridad BAJA**: Optimizar throughput y CER/WER
